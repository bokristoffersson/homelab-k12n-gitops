id: diagnose-crashloop
title: Diagnose CrashLoopBackOff
intent: diagnose
resource: pod
risk_level: none

preconditions:
  - Pod is in CrashLoopBackOff state
  - User has describe and logs permissions

command_template: "kubectl describe pod {{name}} -n {{namespace}}"

flags:
  - name: --show-events
    default: "true"
    note: Include recent events

examples:
  - goal: Diagnose why api pod is crashlooping
    render:
      command: kubectl describe pod api-7d8f6c9b-xk2p9 -n prod
      checks:
        - Look for "Back-off restarting failed container"
        - Check Events section for errors
        - Run kubectl logs api-7d8f6c9b-xk2p9 -n prod --previous

  - goal: Full crashloop investigation workflow
    render:
      command: kubectl describe pod worker-abc123 -n staging
      checks:
        - kubectl logs worker-abc123 -n staging --previous
        - kubectl get events -n staging --sort-by='.lastTimestamp'

postprocess:
  summarize_to_json: false

notes:
  - Check "Last State" for exit code and reason
  - Exit code 0 = clean exit, non-zero = error
  - Exit code 137 = OOMKilled (out of memory)
  - Exit code 1 = application error
  - Always check previous logs with --previous
  - Review resource limits and liveness probes

references:
  - https://kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod/
